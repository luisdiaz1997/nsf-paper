# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .ipy
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
from os import path
# from copy import deepcopy
import numpy as np
# import pandas as pd
from matplotlib import pyplot as plt
from scipy.spatial.distance import pdist,cdist
from scipy.stats.stats import pearsonr,spearmanr
from sklearn.cluster import KMeans
from sklearn import metrics
from scanpy import read_h5ad
from tensorflow_probability import math as tm
tfk = tm.psd_kernels

from utils import training,postprocess,visualize
# from simulations import sim
pth = "simulations/bm_sp"
dpth = path.join(pth,"data")
rpth = path.join(pth,"results")
mpth = path.join(pth,"models")

#%% Benchmarking at command line [markdown]
"""
To run on local computer, use
`python -m simulations.benchmark 2 simulations/bm_sp`
where 2 is a row ID of benchmark.csv, min value 2, max possible value is 115

To run on cluster first load anaconda environment
```
tmux
interactive
module load anaconda3/2021.5
conda activate fwt
python -m simulations.benchmark 2 simulations/bm_sp
```

To run on cluster as a job array, subset of rows. Recommend 10min time limit.
```
PTH=./simulations/bm_sp
sbatch --mem=16G --array=2-26,52-76 ./simulations/benchmark.slurm $PTH
```

To run on cluster as a job array, all rows of CSV file
```
CSV=./simulations/bm_sp/results/benchmark.csv
PTH=./simulations/bm_sp
sbatch --mem=16G --array=1-$(wc -l < $CSV) ./simulations/benchmark.slurm $PTH
```
"""

#%% Load dataset and set kernel and IPs
ad = read_h5ad(path.join(dpth,"S12.h5ad"))
#include only the training observations
Ntr = round(0.95*ad.shape[0])
ad = ad[:Ntr,:]
X = ad.obsm["spatial"]
#extract factors and loadings
tru = postprocess.interpret_nonneg(ad.obsm["spfac"],ad.varm["spload"],lda_mode=False)
F0 = tru["factors"]
W0 = tru["loadings"]
FFd0 = pdist(F0)
WWd0 = pdist(W0)
#set hyperparams
M = 1296
ker = tfk.MaternThreeHalves
hmkw = {"figsize":(6,3),"s":1.5,"marker":"s","subplot_space":0,
        "spinecolor":"gray"}
fig,axes=visualize.multiheatmap(X, F0, (2,4), cmap="Blues", **hmkw)

#%% Compare inferred to true factors
pp = path.join(mpth,"S12/V5/L8/poi_sz-constant/NSF_{}_M{}".format(ker.__name__, M))
tro = training.ModelTrainer.from_pickle(pp)
fit = tro.model
insf = postprocess.interpret_nsf(fit,X)
F = insf["factors"]
fig,axes=visualize.multiheatmap(X, F, (2,4), cmap="Blues", **hmkw)
cdist(F0.T,F.T,metric=lambda x,y: abs(pearsonr(x,y)[0])).max(axis=1)
cdist(F0.T,F.T,metric=lambda x,y: abs(spearmanr(x,y)[0])).max(axis=1)
plt.scatter(F0[:,0],F[:,0])

FFd = pdist(insf["factors"])
W = insf["loadings"]
WWd = pdist(W)
plt.hexbin(FFd0,FFd,gridsize=100,cmap="Greys",bins="log")
plt.scatter(WWd0,WWd)
pearsonr(FFd0,FFd)[0]
spearmanr(FFd0,FFd)[0]
pearsonr(WWd0,WWd)[0]
spearmanr(WWd0,WWd)[0]
nclust = W0.shape[1]
km0 = KMeans(n_clusters=nclust).fit(W0).labels_
km1 = KMeans(n_clusters=nclust).fit(W).labels_
ari = metrics.adjusted_rand_score(km0, km1)

#%% Compute goodness-of-fit metrics
"""
```
python -m simulations.benchmark_gof simulations/bm_sp
```
or
```
PTH=./simulations/bm_sp
sbatch --mem=16G ./simulations/benchmark_gof.slurm $PTH
```
"""

#%% Visualize results
import pandas as pd
import seaborn as sns
from ast import literal_eval
d = pd.read_csv(path.join(rpth,"benchmark.csv"))
# d = d[d["converged"]]
d["factors_pearson"] = d["factors_pearson"].map(lambda x: np.array(literal_eval(x)))
d["factors_pearson_min"] = d["factors_pearson"].map(min)
d["factors_pearson_mean"] = d["factors_pearson"].map(np.mean)
# d["factors_spearman"] = d["factors_spearman"].map(lambda x: np.array(literal_eval(x)))
# d["factors_spearman_min"] = d["factors_spearman"].map(min)
# d["factors_spearman_mean"] = d["factors_spearman"].map(np.mean)
sns.stripplot(x="model",y="factors_pearson_min",hue="sim",dodge=True,data=d)


