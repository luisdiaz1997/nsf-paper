# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .ipy
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

#%%
import pandas as pd
from os import path
from utils import misc,benchmark

pth = "scrna/visium_brain_sagittal"
dpth = path.join(pth,"data")
mpth = path.join(pth,"models/V5")
rpth = path.join(pth,"results")
misc.mkdir_p(rpth)

#%% Create CSV with benchmarking parameters
csv_path = path.join(rpth,"benchmark.csv")
try:
  par = pd.read_csv(csv_path)
except FileNotFoundError:
  L = [6,12,20]
  sp_mods = ["NSF-P", "NSF-N", "NSF-G", "RSF-G", "NSFH-P", "NSFH-N"]
  ns_mods = ["PNMF-P", "PNMF-N", "FA-G"]
  sz = ["constant","scanpy"]
  M = [500,1000,2363]
  V = [5]
  kernels=["MaternThreeHalves"]
  par = benchmark.make_param_df(L,sp_mods,ns_mods,M,sz,V=V,kernels=kernels)
  par.to_csv(csv_path,index=False)

#%% merge old benchmark csv with new
# old = pd.read_csv(path.join(rpth,"benchmark1.csv"))
# new = par.merge(old,on="key",how="outer",copy=True)
# new["converged"] = new["converged_y"]
# new["converged"].fillna(False, inplace=True)
# new.drop(["converged_x","converged_y"],axis=1,inplace=True)
# new.to_csv(path.join(rpth,"benchmark2.csv"),index=False)
##rename benchmark2 to benchmark manually
##some additional scenarios added manually as well (NSF,NSFH with L=36)

#%% Additional scenarios added in response to reviewer comments
# manually merge with original benchmark.csv
# all original scenarios with 80/20 train/val split (only Matern32 kernel, no NB lik)
csv_path = path.join(rpth,"benchmark2.csv")
try:
  par = pd.read_csv(csv_path)
except FileNotFoundError:
  L = [6,12,20]
  sp_mods = ["NSF-P", "RSF-G", "NSFH-P"]
  ns_mods = ["PNMF-P", "FA-G"]
  sz = ["constant","scanpy"]
  M = [500,1000,2363]
  V = [20]
  kernels=["MaternThreeHalves"]
  par = benchmark.make_param_df(L,sp_mods,ns_mods,M,sz,V=V,kernels=kernels)
  par.to_csv(csv_path,index=False)
# NSF, RSF, and NSFH with ExponentiatedQuadratic kernel
# need to manually delete duplicate MEFISTO scenarios in this one
csv_path = path.join(rpth,"benchmark3.csv")
try:
  par = pd.read_csv(csv_path)
except FileNotFoundError:
  L = [6,12,20]
  sp_mods = ["NSF-P", "RSF-G", "NSFH-P"]
  ns_mods = []
  sz = ["constant","scanpy"]
  M = [500,1000,2363]
  V = [5]
  kernels=["ExponentiatedQuadratic"]
  par = benchmark.make_param_df(L,sp_mods,ns_mods,M,sz,V=V,kernels=kernels)
  par.to_csv(csv_path,index=False)

#%% Benchmarking at command line [markdown]
"""
To run on local computer, use
`python -m utils.benchmark 197 scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad`
where 41 is a row ID of benchmark.csv, min value 2, max possible value is 241

To run on cluster first load anaconda environment
```
tmux
interactive
module load anaconda3/2021.5
conda activate fwt
python -m utils.benchmark 14 scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad
```

To run on cluster as a job array, subset of rows, recommend 6hr time limit.
```
DAT=./scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad
sbatch --mem=72G --array=135-196,198-241 ./utils/benchmark_array.slurm $DAT
```

To run on cluster as a job array, all rows of CSV file
```
CSV=./scrna/visium_brain_sagittal/results/benchmark.csv
DAT=./scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad
sbatch --mem=72G --array=2-$(wc -l < $CSV) ./utils/benchmark_array.slurm $DAT
```
"""

#%% Compute metrics for each model (as a job)
"""
DAT=./scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad
sbatch --mem=72G ./utils/benchmark_gof.slurm $DAT 5
#wait until job finishes, then run below
DAT=./scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad
sbatch --mem=72G ./utils/benchmark_gof.slurm $DAT 20
"""

#%% Compute metrics for each model (manually)
from utils import benchmark
dat = "scrna/visium_brain_sagittal/data/visium_brain_sagittal_J2000.h5ad"
res = benchmark.update_results(dat, val_pct=20, todisk=True)

#%% Examine one result
from matplotlib import pyplot as plt
from utils import training
tro = training.ModelTrainer.from_pickle(path.join(mpth,"L4/poi/NSF_MaternThreeHalves_M3000"))
plt.plot(tro.loss["train"][-200:-1])

#%%
csv_file = path.join(rpth,"benchmark.csv")
Ntr = tro.model.Z.shape[0]
benchmark.correct_inducing_pts(csv_file, Ntr)
