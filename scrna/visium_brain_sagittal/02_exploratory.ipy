# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .ipy
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from os import path
from math import ceil
from scanpy import read_h5ad
from tensorflow_probability import math as tm
tfk = tm.psd_kernels

from models import cf,pf,pfh
from models.mefisto import MEFISTO
from utils import preprocess,training,misc,visualize,postprocess

dtp = "float32"
pth = "scrna/visium_brain_sagittal"
dpth = path.join(pth,"data")
mpth = path.join(pth,"models")
rpth = path.join(pth,"results")
plt_pth = path.join(rpth,"plots")

# %% Data Loading from scanpy
J = 2000
ad = read_h5ad(path.join(dpth,"visium_brain_sagittal_J{}.h5ad".format(J)))#[:,:J]
#adtr,adval = preprocess.split_anndata(ad)
#D,fmeans = preprocess.load_data(path.join(dpth,"visium_brain_sagittal_J2000.h5ad"))
Dtr,Dval = preprocess.anndata_to_train_val(ad,layer="counts",sz="scanpy")
Dtr_n,Dval_n = preprocess.anndata_to_train_val(ad) #normalized data
fmeans,Dtr_c,Dval_c = preprocess.center_data(Dtr_n,Dval_n) #centered features
Xtr = Dtr["X"] #note this should be identical to Dtr_n["X"]
Ntr = Xtr.shape[0]
Dtf = preprocess.prepare_datasets_tf(Dtr,Dval=Dval,shuffle=False)
Dtf_n = preprocess.prepare_datasets_tf(Dtr_n,Dval=Dval_n,shuffle=False)
Dtf_c = preprocess.prepare_datasets_tf(Dtr_c,Dval=Dval_c,shuffle=False)
visualize.heatmap(Xtr,Dtr["Y"][:,0],marker="D",s=15)

# %% initialize inducing points and tuning parameters
Z = misc.kmeans_inducing_pts(Xtr, 2363)
M = Z.shape[0]
ker = tfk.MaternThreeHalves
S = 3 #samples for elbo approximation

#%% NPF: Spatial only with non-negative factors
L = 12 #number of latent factors, ideally divisible by 2
try:
  pp = path.join(mpth,"L{}/poi_sz-scanpy/NPF_{}_M{}".format(L,ker.__name__,M))
  tro = training.ModelTrainer.from_pickle(pp)
  fit = tro.model
except FileNotFoundError:
  fit = pf.ProcessFactorization(J,L,Z,psd_kernel=ker,nonneg=True,lik="poi")
  fit.elbo_avg(Xtr,Dtr["Y"],sz=Dtr["sz"])
  fit.init_loadings(Dtr["Y"],X=Xtr,sz=Dtr["sz"])
  fit.elbo_avg(Xtr,Dtr["Y"],sz=Dtr["sz"])
  pp = fit.generate_pickle_path("scanpy",base=mpth)
  tro = training.ModelTrainer(fit,pickle_path=pp)
  %time tro.train_model(*Dtf)
ttl = "NPF: spatial, non-negative factors, Poisson likelihood"
visualize.plot_loss(tro.loss,title=ttl)#,ss=range(2000,4000))
#dev_npf=visualize.gof(fit,Dtr,Dval=Dval,title=ttl)
#%% Postprocessing
hmkw = {"figsize":(4,4), "s":0.3, "marker":"D", "subplot_space":0, 
        "spinecolor":"white"}
inpf = postprocess.interpret_npf(fit,Xtr,S=10,lda_mode=False)
tgnames = [str(i) for i in range(1,L+1)]
fig,axes=visualize.multiheatmap(Xtr, np.sqrt(inpf["factors"]), (4,3), **hmkw)
visualize.set_titles(fig, tgnames, x=0.05, y=.85, fontsize="medium", c="white",
                     ha="left", va="top")
fig.savefig(path.join(plt_pth,"vz_brn_heatmap_npf12.pdf"),bbox_inches='tight')
#%% Top genes for each latent dimensions
W = inpf["loadings"]#*inpf["totals"][:,None]
topgenes = W.argmax(axis=0).tolist()
tgnames = ad.var.index[topgenes]
Ytg = Dtr["Y"][:,topgenes]/Dtr["sz"]
fig,axes=visualize.multiheatmap(Xtr, np.sqrt(Ytg), (4,3), **hmkw)
visualize.set_titles(fig, tgnames, x=0.05, y=.85, fontsize="medium", c="white",
                     ha="left", va="top")
fig.savefig(path.join(plt_pth,"vz_brn_heatmap_npf12_genes.pdf"),bbox_inches='tight')
#save loadings to disk for further interpretation
Wdf=pd.DataFrame(W*inpf["totals"][:,None], index=ad.var.index, columns=range(1,L+1))
Wdf.to_csv(path.join(rpth,"vz_brn_npf12_spde_loadings.csv"))

#%% NCF: Non-spatial, nonnegative
L = 12
try:
  pp = path.join(mpth,"L{}/poi_sz-scanpy/NCF".format(L))
  tro = training.ModelTrainer.from_pickle(pp)
  fit = tro.model
except FileNotFoundError:
  fit = cf.CountFactorization(Ntr, J, L, lik="poi", nonneg=True)
  fit.elbo_avg(Dtr["Y"],sz=Dtr["sz"],idx=Dtr["idx"])
  fit.init_loadings(Dtr["Y"],sz=Dtr["sz"])
  pp = fit.generate_pickle_path("scanpy",base=mpth)
  tro = training.ModelTrainer(fit,pickle_path=pp)
  %time tro.train_model(*Dtf)
ttl = "NCF: nonspatial, non-negative factors, Poisson likelihood"
visualize.plot_loss(tro.loss,title=ttl)#,ss=range(2000,4000))
#dev_ncf = visualize.gof(fit,Dtr,Dval=Dval,title=ttl)
#%% Postprocess
hmkw = {"figsize":(6,4), "s":0.3, "marker":"D", "subplot_space":0, 
        "spinecolor":"white"}
incf = postprocess.interpret_ncf(fit,S=10,lda_mode=False)
tgnames = [str(i) for i in range(1,L+1)]
fig,axes=visualize.multiheatmap(Xtr, np.sqrt(incf["factors"]), (3,4), **hmkw)
visualize.set_titles(fig, tgnames, x=0.05, y=.85, fontsize="medium", c="white",
                     ha="left", va="top")
fig.savefig(path.join(plt_pth,"vz_brn_heatmap_ncf12.pdf"),bbox_inches='tight')

#%% NPF Hybrid object
L = 20
try:
  T = ceil(L/2)
  pp = path.join(pth,"models/L{}/poi_sz-scanpy/NPFH_T{}_{}_M{}".format(L,T,ker.__name__, M))
  tro = training.ModelTrainer.from_pickle(pp)
  fit = tro.model
except FileNotFoundError:
  fit = pfh.ProcessFactorizationHybrid(Ntr, J, L, Z, lik="poi", nonneg=True,
                                       psd_kernel=ker)
  fit.elbo_avg(Dtr["X"],Dtr["Y"],Dtr["idx"])
  fit.init_loadings(Dtr["Y"],X=Dtr["X"])
  pp = fit.generate_pickle_path("scanpy",base=mpth)
  tro = training.ModelTrainer(fit,pickle_path=pp)
  %time tro.train_model(*Dtf, S=S) #1hr
ttl = "NPFH: spatial, non-negative factors, Poisson likelihood"
visualize.plot_loss(tro.loss,title=ttl)#,ss=range(200,240))
#dev_npfh =visualize.gof(fit,Dtr,Dval=Dval,title=ttl)

# %% NPFH: Postprocess SPDE style
hmkw = {"figsize":(10,4), "s":0.5, "marker":"D", "subplot_space":0, 
        "spinecolor":"white"}
inpfh = postprocess.interpret_npfh(fit,Xtr,S=10,lda_mode=False)
tgnames = [str(i) for i in range(1,T+1)]
fig,axes=visualize.multiheatmap(Xtr, np.sqrt(inpfh["spatial"]["factors"]),
                                (2,5), **hmkw)
visualize.set_titles(fig, tgnames, x=0.05, y=.85, fontsize="medium", c="white",
                     ha="left", va="top")
fig.savefig(path.join(plt_pth,"vz_brn_heatmap_npfh{}_spde_spat.pdf".format(L)),
            bbox_inches='tight')
fig,axes=visualize.multiheatmap(Xtr, np.sqrt(inpfh["nonspatial"]["factors"]),
                                (2,5),**hmkw)
visualize.set_titles(fig, tgnames, x=0.05, y=.85, fontsize="medium", c="white",
                     ha="left", va="top")
fig.savefig(path.join(plt_pth,"vz_brn_heatmap_npfh{}_spde_nsp.pdf".format(L)),
            bbox_inches='tight')
#%% Top genes for each latent dimension
W = inpfh["spatial"]["loadings"]#*inpf["totals"][:,None]
W[np.isnan(W)] = 0.0
V = inpfh["nonspatial"]["loadings"]
V[np.isnan(V)] = 0.0
topgenes = W.argmax(axis=0).tolist()
tgnames = ad.var.index[topgenes]
Ytg = Dtr["Y"][:,topgenes]/Dtr["sz"]
fig,axes=visualize.multiheatmap(Xtr, np.sqrt(Ytg), (2,5), **hmkw)
visualize.set_titles(fig, tgnames, x=0.05, y=.85, fontsize="medium", c="white",
                     ha="left", va="top")
fig.savefig(path.join(plt_pth,
                      "vz_brn_heatmap_npfh{}_spde_spat_genes.pdf".format(L)),
            bbox_inches='tight')
#save loadings to disk for further interpretation
WV = np.hstack((W,V))*inpfh["totals"][:,None]
WVdf=pd.DataFrame(WV, index=ad.var.index, columns=range(1,L+1))
WVdf.to_csv(path.join(rpth,"vz_brn_npfh{}_spde_loadings.csv".format(L)))
#%% spatial importance per gene
alpha = inpfh["spatial"]["loadings"].sum(axis=1)
pd1 = pd.DataFrame({"gene":ad.var_names, "data":"visium_brain_sagittal",
                    "L":L, "T":T, "spatial_wt":alpha})
pd1.to_csv(path.join(rpth,"NPFH_spatial_gene_weights_L{}_T{}.csv".format(L,T)),
           index=False)
pd1.spatial_wt.hist(bins=100)
# %% relative importance of each factor
pd2 = postprocess.npfh_factor_importance(inpfh,lda_mode=False)
pd2.plot.bar(x="factor_type",y="weight")
pd2.to_csv(path.join(rpth,"NPFH_dim_weights_spde_L{}_T{}.csv".format(L,T)),
           index=False)
# %% Does implied size factor correspond to real size factor
plt.scatter(Dtr["Y"].sum(axis=0),inpfh["totals"])
plt.axline((0,0),slope=1,c="black",ls="--",lw=2)

# %% NPFH: Postprocess LDA style
inpfh = postprocess.interpret_npfh(fit,Xtr,S=10,lda_mode=True)
# fig,axes=visualize.multiheatmap(Xtr, np.sqrt(inpfh["spatial"]["factors"]),
#                                 (3,6), **hmkw)
# fig.savefig(path.join(plt_pth,"vz_brn_heatmap_npfh36_lda_spat.pdf"),bbox_inches='tight')
# fig,axes=visualize.multiheatmap(Xtr, np.sqrt(inpfh["nonspatial"]["factors"]),
#                                 (3,6), **hmkw)
# fig.savefig(path.join(plt_pth,"vz_brn_heatmap_npfh36_lda_nsp.pdf"),bbox_inches='tight')
#%% spatial importance per cell
alpha = inpfh["spatial"]["factors"].sum(axis=1)
pd1 = pd.DataFrame({"data":"visium_brain_sagittal", "L":L, "T":T, "spatial_wt":alpha})
pd1.spatial_wt.hist(bins=100)
pd1.to_csv(path.join(rpth,"NPFH_spatial_cell_weights_L{}_T{}.csv".format(L,T)),
           index=False)
# %% relative importance of each factor
pd2 = postprocess.npfh_factor_importance(inpfh,lda_mode=True)
pd2.plot.bar(x="factor_type",y="weight")
pd2.to_csv(path.join(rpth,"NPFH_dim_weights_lda_L{}_T{}.csv".format(L,T)),
           index=False)

#%%Gaussian NPF
fit = pf.ProcessFactorization(J,L,Z,psd_kernel=ker,nonneg=True,lik="gau")
fit.init_loadings(Dtr_n["Y"],X=Dtr_n["X"])
pp = fit.generate_pickle_path(base=mpth)
tro = training.ModelTrainer(fit,pickle_path=pp)
%time tro.train_model(*Dtf_n)
ttl = "NPF: spatial, non-negative factors, Gaussian likelihood"
visualize.plot_loss(tro.loss,title=ttl)#,ss=range(2000,4000))
dev_npfg = visualize.gof(fit,Dtr,Dval=Dval,title=ttl)

#%%Gaussian RPF
fit = pf.ProcessFactorization(J,L,Z,psd_kernel=ker,nonneg=False,lik="gau",
                              feature_means=fmeans)
fit.init_loadings(Dtr_c["Y"])
pp = fit.generate_pickle_path(base=mpth)
tro = training.ModelTrainer(fit,pickle_path=pp)
%time tro.train_model(*Dtf_c)
ttl = "RPF: spatial, real-valued factors, Gaussian likelihood"
visualize.plot_loss(tro.loss,title=ttl)#,ss=range(2000,4000))
dev_rpf = visualize.gof(fit,Dtr,Dval=Dval,title=ttl)

# %% MEFISTO- Gaussian
pp = path.join(pth,"models/L4/gau/MEFISTO_M1000")
mef = MEFISTO(Dtr_n, 4, inducing_pts=1000, pickle_path=pp)
%time mef.train() #also saves to pickle file
#mef = MEFISTO.from_pickle(pp)
ttl = "MEFISTO"
dev_mef = visualize.gof(mef,Dtr,Dval=Dval,title=ttl)

#%% RCF: Non-spatial, real-valued
fit = cf.CountFactorization(Ntr, J, L, lik="gau", nonneg=False,
                            feature_means=fmeans)
fit.init_loadings(Dtr_c["Y"])
pp = fit.generate_pickle_path(base=mpth)
tro = training.ModelTrainer(fit,pickle_path=pp)
%time tro.train_model(*Dtf_c)
ttl = "NCF: nonspatial, non-negative factors"
visualize.plot_loss(tro.loss,title=ttl)
dev_rcf = visualize.gof(fit,Dtr,Dval=Dval,title=ttl)

#%% NMF from sklearn
from sklearn.decomposition import NMF
fit = NMF(L,beta_loss="kullback-leibler",solver="mu")
Fhat = fit.fit_transform(Dtr["Y"])
visualize.multiheatmap(Xtr,Fhat,(3,4),figsize=(7,4),s=1,marker="D",cmap="Blues")

#%% PCA from sklearn
from sklearn.decomposition import PCA
fit = PCA(L)
Fhat = fit.fit_transform(Dtr["Y"])
visualize.multiheatmap(Xtr,Fhat,(3,4),figsize=(7,4),s=1,marker="D",cmap="RdBu")
